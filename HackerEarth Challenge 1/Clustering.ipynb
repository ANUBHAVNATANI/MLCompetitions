{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 3 columns):\n",
      "id          3000 non-null object\n",
      "headline    3000 non-null object\n",
      "text        3000 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 70.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uid-1</td>\n",
       "      <td>Market Advances 5.12 More</td>\n",
       "      <td>NEW YORK (AP) - A prime rate reduction by New ...</td>\n",
       "      <td>Market Advances 5.12 More NEW YORK (AP) - A pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uid-2</td>\n",
       "      <td>District Boosts Request For Anti-Terrorism Aid...</td>\n",
       "      <td>Mayor Anthony A. Williams petitioned the White...</td>\n",
       "      <td>District Boosts Request For Anti-Terrorism Aid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uid-3</td>\n",
       "      <td>Election? Here's How You Do It, Mate.</td>\n",
       "      <td>From our downunder perspective here in Austral...</td>\n",
       "      <td>Election? Here's How You Do It, Mate. From our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uid-4</td>\n",
       "      <td>The Biggest Boom Ever</td>\n",
       "      <td>We are about to rewrite history. Unless a rece...</td>\n",
       "      <td>The Biggest Boom Ever We are about to rewrite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uid-5</td>\n",
       "      <td>Economic Aide Sees Uptrend</td>\n",
       "      <td>Sedate and scholarly Dr. Arthur Burns, the ex-...</td>\n",
       "      <td>Economic Aide Sees Uptrend Sedate and scholarl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           headline  \\\n",
       "0  uid-1                          Market Advances 5.12 More   \n",
       "1  uid-2  District Boosts Request For Anti-Terrorism Aid...   \n",
       "2  uid-3              Election? Here's How You Do It, Mate.   \n",
       "3  uid-4                              The Biggest Boom Ever   \n",
       "4  uid-5                         Economic Aide Sees Uptrend   \n",
       "\n",
       "                                                text  \\\n",
       "0  NEW YORK (AP) - A prime rate reduction by New ...   \n",
       "1  Mayor Anthony A. Williams petitioned the White...   \n",
       "2  From our downunder perspective here in Austral...   \n",
       "3  We are about to rewrite history. Unless a rece...   \n",
       "4  Sedate and scholarly Dr. Arthur Burns, the ex-...   \n",
       "\n",
       "                                                 new  \n",
       "0  Market Advances 5.12 More NEW YORK (AP) - A pr...  \n",
       "1  District Boosts Request For Anti-Terrorism Aid...  \n",
       "2  Election? Here's How You Do It, Mate. From our...  \n",
       "3  The Biggest Boom Ever We are about to rewrite ...  \n",
       "4  Economic Aide Sees Uptrend Sedate and scholarl...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"news.csv\")\n",
    "data.info()\n",
    "data[\"new\"] = data[\"headline\"].map(str)+\" \"+ data[\"text\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubh\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "desc = data['headline'].values\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]\n",
    "#vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "vectorizer3 = TfidfVectorizer(tokenizer = tokenize,stop_words=stop_words,max_features=1000)\n",
    "X3 = vectorizer3.fit_transform(desc)\n",
    "words = vectorizer3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubh\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "desc = data['headline'].values\n",
    "def tokenize(text):\n",
    "    return [porter.stem(word) for word in tokenizer.tokenize(text.lower())]\n",
    "#making the proper mtd to make vectorized words\n",
    "vectorizer3 = TfidfVectorizer(tokenizer = tokenize,stop_words=stop_words,max_features=1000)\n",
    "X3 = vectorizer3.fit_transform(desc)\n",
    "words = vectorizer3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : new, econom, recess, plan, trade, report, profit, fund, deficit, year, time, billion, bank, sale, say, gain, c, pay, hous, budget, jobless, growth, wall, firm, money\n",
      "1 : tax, cut, plan, budget, deficit, summari, break, senat, propos, develop, forecast, way, benefit, state, feder, special, rais, capit, oppos, warn, approv, fair, spend, clear, busi\n",
      "2 : rise, rate, stock, dow, profit, market, economist, gain, month, fed, oil, fall, sale, tech, straight, predict, say, industri, increas, boost, quarter, point, benefit, jobless, labor\n",
      "3 : fed, rate, polici, money, bank, action, say, treasuri, new, greenspan, credit, cut, bond, york, market, minut, year, offici, report, volcker, vote, chairman, stock, growth, expect\n",
      "4 : stock, gain, trade, investor, sell, focu, small, week, mix, buy, report, dow, drop, tech, slump, close, end, lift, ralli, chip, fall, higher, new, rail, bond\n",
      "5 : rate, fall, mortgag, key, low, jobless, cut, fix, loan, dow, dip, declin, bond, lower, bank, year, seen, fear, market, higher, auction, drop, hit, term, long\n",
      "6 : world, news, watch, corpor, busi, bad, imf, let', climb, post, good, factori, credit, peril, china', eu, apprais, ignor, real, japan, what', want, wide, industri, bank\n",
      "7 : financ, busi, yuan, ford, forc, food, follow, focus, focu, flood, flat, fix, fiscal, firm, finish, financi, final, foreign, file, figur, fight, fifth, field, fewer, fell\n",
      "8 : market, stock, ralli, gain, advanc, trade, week, street, declin, dow, moder, year, slow, higher, trader, report, say, fear, rebound, close, end, low, blue, new, bond\n",
      "9 : record, stock, set, hit, high, trade, dow, deficit, rise, gain, surg, year, issu, london, ralli, industri, import, billion, export, turnov, profit, point, european, treasuri, tokyo\n",
      "10 : digest, page, dow, yuan, flat, finish, firm, fiscal, fix, flood, financ, focu, focus, follow, food, forc, financi, file, final, forecast, figur, fight, fifth, field, fewer\n",
      "11 : washington, post, thursday, octob, r, wednesday, tuesday, januari, juli, e, d, march, saturday, l, sunday, septemb, busi, august, g, friday, c, current, file, write, brief\n",
      "12 : price, bond, rise, stock, oil, wholesal, market, trade, inflat, produc, gain, fall, energi, rate, ralli, data, food, declin, higher, index, dow, littl, jump, rais, increas\n",
      "13 : job, dow, report, data, gain, year, treasuri, unemploy, index, week, news, growth, point, odd, sell, econom, surg, follow, investor, cut, high, fall, privat, matter, sector\n",
      "14 : dollar, yen, declin, fed, mark, euro, currenc, eas, rate, bank, intervent, data, central, lose, growth, mix, despit, defi, recov, strong, high, gold, trade, face, expect\n",
      "15 : economi, track, say, growth, strong, grow, spend, fed, rate, new, report, slow, cite, sign, pick, bush, whi, rise, outlook, boost, fell, consum, thi, insid, signal\n",
      "16 : big, fund, stock, investor, financi, bond, year, steel, product, bad, issu, win, mani, case, global, bank, time, ha, gain, small, expect, money, averag, player, market\n",
      "17 : s, u, news, rate, economi, deficit, capit, say, trade, dollar, job, end, aid, korea, bank, econom, market, japan, gain, budget, plan, foreign, inflat, growth, increas\n",
      "18 : tape, ahead, week, plan, yuan, flat, finish, firm, fiscal, fix, flood, financ, focu, focus, follow, food, forc, financi, final, forecast, file, figur, fight, fifth, field\n",
      "19 : inflat, fear, anti, fed', worri, continu, carter, say, index, rate, outpac, ha, goal, price, fight, propos, hous, hedg, food, fed, cut, china, view, impact, consum\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndbscan = DBSCAN(algorithm='brute',n_jobs=3)\\ndbscan.fit(X3)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "k=20\n",
    "\n",
    "#kmeans = MiniBatchKMeans(n_clusters=k, init='k-means++', n_init=1, random_state=42,init_size=1000, verbose=False, max_iter=1000)\n",
    "kmeans = KMeans(n_clusters=k,n_jobs=3)\n",
    "kmeans.fit(X3)\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "dbscan = DBSCAN(algorithm='brute',n_jobs=3)\n",
    "dbscan.fit(X3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndarray = X3.todense()\n",
    "#c = dbscan.labels_|\n",
    "#c = kmeans.predict(X3)\n",
    "c = kmeans.labels_\n",
    "dataf = {'id':data['id'].values,'cluster':c}\n",
    "datafr = pd.DataFrame(data=dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05250301200691526"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score\n",
    "from sklearn.metrics import silhouette_score as sc\n",
    "sc(ndarray,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafr.to_csv('sub.csv',index=False)\n",
    "np.savetxt('sub.txt', ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file saved sucessfully\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile \n",
    "import os \n",
    "with ZipFile('submit.zip','w') as zip:\n",
    "    zip.write('sub.csv')\n",
    "    zip.write('sub.txt')\n",
    "print(\"Zip file saved sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
